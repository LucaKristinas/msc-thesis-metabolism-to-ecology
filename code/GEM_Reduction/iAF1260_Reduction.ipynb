{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library and 3rd Party Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import copy\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COBRA imports\n",
    "\n",
    "import cobra\n",
    "from cobra import Model, Reaction\n",
    "from cobra.core import Group, Reaction\n",
    "from cobra.flux_analysis import flux_variability_analysis\n",
    "from cobra.flux_analysis.fastcc import fastcc\n",
    "from cobra.io import read_sbml_model, save_matlab_model, write_sbml_model\n",
    "from cobra.util.solver import linear_reaction_coefficients\n",
    "from rapidfuzz import fuzz, process\n",
    "from networkx.algorithms import bipartite\n",
    "from scipy.linalg import svd\n",
    "from cobra.util.array import create_stoichiometric_matrix\n",
    "from networkx.algorithms.simple_paths import shortest_simple_paths\n",
    "from optlang.symbolics import Zero\n",
    "from optlang import Model as OptModel\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_reactions_interconnected(model, reaction_ids):\n",
    "    \"\"\"\n",
    "    Check whether a list of reactions are all interconnected via shared metabolites.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "    - reaction_ids: list of reaction IDs to test\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if all reactions are connected via shared metabolites\n",
    "    \"\"\"\n",
    "    # Create a graph where nodes = reactions, edges = shared metabolite\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(reaction_ids)\n",
    "\n",
    "    # Build edges based on shared metabolites\n",
    "    for i, rxn1_id in enumerate(reaction_ids):\n",
    "        if rxn1_id not in model.reactions:\n",
    "            continue\n",
    "        rxn1 = model.reactions.get_by_id(rxn1_id)\n",
    "        mets1 = set(rxn1.metabolites)\n",
    "\n",
    "        for rxn2_id in reaction_ids[i+1:]:\n",
    "            if rxn2_id not in model.reactions:\n",
    "                continue\n",
    "            rxn2 = model.reactions.get_by_id(rxn2_id)\n",
    "            mets2 = set(rxn2.metabolites)\n",
    "\n",
    "            # Add edge if they share at least one metabolite\n",
    "            if mets1 & mets2:\n",
    "                G.add_edge(rxn1_id, rxn2_id)\n",
    "\n",
    "    # Check if the graph is fully connected\n",
    "    return nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_connected_subgroup(model, reaction_list):\n",
    "    \"\"\"\n",
    "    Identify the largest connected component of a group of reactions based on shared metabolites.\n",
    "\n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "        A COBRA model containing reactions and metabolites.\n",
    "    - reaction_list: list of str\n",
    "        A list of reaction IDs to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - list of str:\n",
    "        The subset of reaction IDs from the input that belong to the largest connected component.\n",
    "        Connectivity is defined by reactions sharing at least one metabolite (bipartite graph: reactions â†” metabolites).\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for rxn_id in reaction_list:\n",
    "        rxn = model.reactions.get_by_id(rxn_id)\n",
    "        for met in rxn.metabolites:\n",
    "            G.add_edge(rxn.id, met.id)\n",
    "\n",
    "    components = nx.connected_components(G)\n",
    "    \n",
    "    reaction_components = [\n",
    "        set(comp).intersection(reaction_list) for comp in components\n",
    "    ]\n",
    "    \n",
    "    largest = max(reaction_components, key=len, default=[])\n",
    "    return list(largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lump_reaction(model, coupled_df, reaction_column=\"Coupled_Reactions\", verbose=True, label_type=\"Group\"):\n",
    "    \"\"\"\n",
    "    Create a lumped model by replacing fully coupled reaction groups with pseudo-reactions.\n",
    "\n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "        The original COBRA model (remains unchanged).\n",
    "    - coupled_df: pd.DataFrame\n",
    "        DataFrame with a column containing lists of reactions to lump.\n",
    "    - reaction_column: str\n",
    "        The column name in `coupled_df` that holds lists of reactions (default: \"Coupled_Reactions\").\n",
    "    - verbose: bool\n",
    "        If True, print progress messages.\n",
    "    - label_type: str\n",
    "        Label prefix for naming the pseudo-reactions (e.g., \"Group\", \"Module\").\n",
    "\n",
    "    Returns:\n",
    "    - model_lumped: cobra.Model\n",
    "        A copy of the model with pseudo-reactions added and original reactions removed.\n",
    "    - translation_df: pd.DataFrame\n",
    "        DataFrame mapping pseudo-reactions to their original reaction IDs.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    from cobra import Reaction\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "\n",
    "    model_lumped = copy.deepcopy(model)\n",
    "    translation_data = []\n",
    "\n",
    "    for i, row in coupled_df.iterrows():\n",
    "        group_reactions = row[reaction_column]\n",
    "\n",
    "        # Filter only reactions that exist in the model\n",
    "        valid_reactions = [rxn_id for rxn_id in group_reactions if rxn_id in model_lumped.reactions]\n",
    "\n",
    "        if len(valid_reactions) < 2:\n",
    "            if verbose:\n",
    "                print(f\"â­ï¸ Skipping group {i} â€” fewer than 2 valid reactions.\")\n",
    "            continue\n",
    "\n",
    "        # Determine common flux bounds\n",
    "        lower_bounds = [model_lumped.reactions.get_by_id(r).lower_bound for r in valid_reactions]\n",
    "        upper_bounds = [model_lumped.reactions.get_by_id(r).upper_bound for r in valid_reactions]\n",
    "        min_lb = max(lower_bounds)\n",
    "        max_ub = min(upper_bounds)\n",
    "\n",
    "        # Compute net stoichiometry\n",
    "        net_stoich = defaultdict(float)\n",
    "        for rxn_id in valid_reactions:\n",
    "            rxn = model_lumped.reactions.get_by_id(rxn_id)\n",
    "            for met, coeff in rxn.metabolites.items():\n",
    "                net_stoich[met] += coeff\n",
    "        cleaned_stoich = {met: coeff for met, coeff in net_stoich.items() if abs(coeff) > 1e-10}\n",
    "\n",
    "        # Create pseudo-reaction\n",
    "        pseudo_id = f\"Pseudo_{label_type}_{i}\"\n",
    "        pseudo_rxn = Reaction(id=pseudo_id)\n",
    "        pseudo_rxn.name = f\"Lumped reaction for: {', '.join(valid_reactions)}\"\n",
    "        pseudo_rxn.lower_bound = min_lb\n",
    "        pseudo_rxn.upper_bound = max_ub\n",
    "        pseudo_rxn.add_metabolites(cleaned_stoich)\n",
    "\n",
    "        # Add and remove reactions\n",
    "        model_lumped.add_reactions([pseudo_rxn])\n",
    "        for rxn_id in valid_reactions:\n",
    "            model_lumped.reactions.remove(model_lumped.reactions.get_by_id(rxn_id))\n",
    "\n",
    "        # Record mapping\n",
    "        translation_data.append({\n",
    "            'Pseudo_Reaction_ID': pseudo_id,\n",
    "            'Original_Reactions': valid_reactions\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"âœ… {label_type} {i}: Created '{pseudo_id}' from {valid_reactions}\")\n",
    "\n",
    "    translation_df = pd.DataFrame(translation_data)\n",
    "    return model_lumped, translation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_precursors(model, biomass_precursors, core_mets, max_depth=5):\n",
    "    \"\"\"\n",
    "    Find core metabolites that can act as precursors to each biomass precursor,\n",
    "    using reactions in the model. Handles reversibility and stoichiometry.\n",
    "\n",
    "    Parameters:\n",
    "        model (cobra.Model): The metabolic model.\n",
    "        biomass_precursors (list): List of metabolite IDs in the biomass reaction.\n",
    "        core_mets (list): List of core metabolite IDs.\n",
    "        max_depth (int): Maximum number of reactions to search backwards.\n",
    "\n",
    "    Returns:\n",
    "        dict: {biomass_precursor: set of core_metabolites_that_can_reach_it}\n",
    "    \"\"\"\n",
    "    # Build reverse graph: met_id -> reactions that can produce it\n",
    "    met_to_producing_rxns = defaultdict(list)\n",
    "\n",
    "    for rxn in model.reactions:\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            if coeff > 0:\n",
    "                # Metabolite is produced in forward direction\n",
    "                met_to_producing_rxns[met.id].append((rxn, 'forward'))\n",
    "            elif coeff < 0 and rxn.lower_bound < 0:\n",
    "                # Metabolite is produced in reverse direction\n",
    "                met_to_producing_rxns[met.id].append((rxn, 'reverse'))\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for target_met in biomass_precursors:\n",
    "        visited_mets = set()\n",
    "        visited_rxns = set()\n",
    "        queue = deque([(target_met, 0)])  # (metabolite_id, depth)\n",
    "        precursors = set()\n",
    "\n",
    "        while queue:\n",
    "            current_met, depth = queue.popleft()\n",
    "            if depth >= max_depth:\n",
    "                continue\n",
    "\n",
    "            producing_rxns = met_to_producing_rxns.get(current_met, [])\n",
    "\n",
    "            for rxn, direction in producing_rxns:\n",
    "                if (rxn.id, direction) in visited_rxns:\n",
    "                    continue\n",
    "                visited_rxns.add((rxn.id, direction))\n",
    "\n",
    "                # Get reactants based on direction\n",
    "                if direction == 'forward':\n",
    "                    reactants = rxn.reactants\n",
    "                else:\n",
    "                    reactants = rxn.products  # in reverse, products become precursors\n",
    "\n",
    "                for reactant in reactants:\n",
    "                    if reactant.id in visited_mets:\n",
    "                        continue\n",
    "                    visited_mets.add(reactant.id)\n",
    "\n",
    "                    if reactant.id in core_mets:\n",
    "                        precursors.add(reactant.id)\n",
    "                    else:\n",
    "                        queue.append((reactant.id, depth + 1))\n",
    "\n",
    "        result[target_met] = precursors\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_unreachable_precursors(model, biomass_precursors, core_mets, max_depth_range=10):\n",
    "    \"\"\"\n",
    "    For each depth level (1 to max_depth_range), report biomass precursors \n",
    "    that cannot be reached from core metabolites.\n",
    "\n",
    "    Parameters:\n",
    "        model (cobra.Model): COBRApy model.\n",
    "        biomass_precursors (list): List of biomass metabolite IDs.\n",
    "        core_mets (list): List of core metabolite IDs.\n",
    "        max_depth_range (int): Max depth to explore (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        dict: {depth: list of unreachable biomass precursors at that depth}\n",
    "    \"\"\"\n",
    "    depth_to_unreachable = {}\n",
    "\n",
    "    for depth in range(1, max_depth_range + 1):\n",
    "        precursor_map = find_precursors(model, biomass_precursors, core_mets, max_depth=depth)\n",
    "        unreachable = [met for met, precursors in precursor_map.items() if not precursors]\n",
    "        depth_to_unreachable[depth] = unreachable\n",
    "        print(f\"Step {depth}: {len(unreachable)} precursors not reachable â†’ {unreachable}\")\n",
    "\n",
    "    return depth_to_unreachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project root path by going up from the notebook location\n",
    "notebook_dir = Path(__file__).parent if '__file__' in globals() else Path().resolve()\n",
    "project_root = notebook_dir.parent.parent  # Go up from /code/GEM_Reduction/\n",
    "\n",
    "# Construct raw paths \n",
    "raw_data_path = project_root / \"data\" / \"raw\" \n",
    "raw_sbml_path = raw_data_path / \"sbml_files\" \n",
    "raw_mat_path = raw_data_path / \"matlab_files\" \n",
    "raw_csv_path = raw_data_path / \"csv_files\" \n",
    "\n",
    "# Construct processed paths \n",
    "processed_data_path = project_root / \"data\" / \"processed\" \n",
    "processed_sbml_path = processed_data_path / \"sbml_files\" \n",
    "processed_csv_path = processed_data_path / \"csv_files\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model\n",
    "model = read_sbml_model(raw_sbml_path / \"iAF1260.xml\")\n",
    "subsystem_df = pd.read_csv(raw_csv_path / 'iAF1260_subsystem_assignments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pairs coupled and reactions blocked by F2C2\n",
    "Coupled_Pairs_df = pd.read_csv(raw_csv_path / 'fctable_iAF1260.csv', header=None)\n",
    "F2C2_Blocked_Reactions_df = pd.read_csv(raw_csv_path / 'blocked_reactions_iAF1260.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Basics\n",
    "- Model ID: iAF1260\n",
    "- Number of Reactions: 2382\n",
    "- Number of Metabolites: 1668\n",
    "- Number of Genes: 1261\n",
    "- Number of Exchange Reactions: 299\n",
    "- Reversible Reactions: 575\n",
    "- Irreversible Reactions: 1807\n",
    "- Objective reaction(s): R_BIOMASS_Ec_iAF1260_core_59p81M\n",
    "\n",
    "- FBA Objective value (biomass): 0.7367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 1630\n",
      "Number of Reactions: 2382\n",
      "Degrees of Freedom: 752\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(model)  # Already a NumPy array\n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(model.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a deep copy of the model to avoid modifying the original\n",
    "model_copy = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Open flux bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow flux in and out for all exchange reactions\n",
    "for rxn in model_copy.exchanges:\n",
    "    rxn.lower_bound = -10\n",
    "    rxn.upper_bound = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After opening bounds of all exchanges, the optimal solution is 36.4!\n"
     ]
    }
   ],
   "source": [
    "solution_open = model_copy.optimize()\n",
    "obj_open = solution_open.objective_value\n",
    "\n",
    "print(f'After opening bounds of all exchanges, the optimal solution is {round(obj_open,2)}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export as .mat for matlab applications\n",
    "\n",
    "save_matlab_model(model_copy, raw_mat_path / \"iAF1260.mat\") # use generic model to get unbiased couplings (filter later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Run FASTCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce model with COBRApy FASTCC \n",
    "consistent_generic_model = fastcc(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 1440\n",
      "Number of Reactions: 2153\n",
      "Degrees of Freedom: 713\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(consistent_generic_model)  # Already a NumPy array\n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(consistent_generic_model.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1**: Make F2C2 Data accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reaction annotations to match F2C2 blocked reactions\n",
    "model_rxns = [rxn.id for rxn in model.reactions]\n",
    "F2C2_Blocked_Reactions_df.loc[len(F2C2_Blocked_Reactions_df)] = model_rxns\n",
    "\n",
    "# Identify unblocked reactions from the second row (index 1)\n",
    "unblocked_mask = F2C2_Blocked_Reactions_df.loc[0] == 0\n",
    "unblocked_reactions = F2C2_Blocked_Reactions_df.loc[1][unblocked_mask].tolist()\n",
    "\n",
    "# Update Coupled_Pairs_df index and columns with unblocked reactions\n",
    "Coupled_Pairs_df.index = unblocked_reactions\n",
    "Coupled_Pairs_df.columns = unblocked_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of how many fully coupled pairs\n",
    "fully_coupled_count = int(((Coupled_Pairs_df == 1).sum().sum()) - len(Coupled_Pairs_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SideQuest* : Investigate difference in 'blocking' between FASTCC and F2C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and convert reactions from models to sets for fast comparison\n",
    "generic_rxns = {rxn.id for rxn in consistent_generic_model.reactions}\n",
    "f2c2_unblocked_rxns = set(unblocked_reactions)\n",
    "\n",
    "# Compare overlaps and differences\n",
    "overlap_generic = list(generic_rxns & f2c2_unblocked_rxns)\n",
    "\n",
    "# Reactions unblocked by F2C2 but removed by FASTCC\n",
    "missing_from_generic = list(f2c2_unblocked_rxns - generic_rxns)\n",
    "\n",
    "# Reactions kept by FASTCC but blocked by F2C2 (ideally empty)\n",
    "unexpected_in_generic = list(generic_rxns - f2c2_unblocked_rxns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Create 'Fully Coupled Clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph of fully coupled reactions\n",
    "G = nx.Graph()\n",
    "\n",
    "for row in Coupled_Pairs_df.index:\n",
    "    for col in Coupled_Pairs_df.columns:\n",
    "        if Coupled_Pairs_df.loc[row, col] == 1 and row != col:\n",
    "            G.add_edge(row, col)\n",
    "\n",
    "# Find connected components (fully coupled groups)\n",
    "coupled_groups = list(nx.connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reaction groups with net stoichiometry and input/output metabolites\n",
    "combined_data = []\n",
    "for group in coupled_groups:\n",
    "    group_reactions = list(group)\n",
    "\n",
    "    # Compute net stoichiometry\n",
    "    stoich = defaultdict(float)\n",
    "    for rxn_id in group_reactions:\n",
    "        rxn = model.reactions.get_by_id(rxn_id)\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            stoich[met] += coeff\n",
    "\n",
    "    inputs = [met.id for met, coeff in stoich.items() if coeff < 0]\n",
    "    outputs = [met.id for met, coeff in stoich.items() if coeff > 0]\n",
    "\n",
    "    combined_data.append({\n",
    "        \"Coupled_Reactions\": group_reactions,\n",
    "        \"Num_Inputs\": len(inputs),\n",
    "        \"Num_Outputs\": len(outputs),\n",
    "        \"Input_Metabolites\": inputs,\n",
    "        \"Output_Metabolites\": outputs\n",
    "    })\n",
    "\n",
    "# Create initial DataFrame\n",
    "Fully_Coupled_df = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'EX_' reactions removed: 277\n",
      "'BIOMASS_Ec_iAF1260_core_59p81M' was removed: True\n",
      "Total reactions removed due to fastcc removal: 35\n"
     ]
    }
   ],
   "source": [
    "# Flatten reaction lists to count how many should be removed\n",
    "all_reactions = Fully_Coupled_df['Coupled_Reactions'].explode()\n",
    "\n",
    "ex_removed_count = all_reactions.str.startswith('EX_').sum()\n",
    "biomass_removed_flag = 'BIOMASS_Ec_iAF1260_core_59p81M' in set(all_reactions)\n",
    "missing_set = set(missing_from_generic)\n",
    "missing_removed_count = all_reactions.isin(missing_set).sum()\n",
    "\n",
    "# Clean the reaction lists: remove EX_, biomass, and missing reactions\n",
    "Fully_Coupled_df['Coupled_Reactions'] = Fully_Coupled_df['Coupled_Reactions'].apply(\n",
    "    lambda rxns: [\n",
    "        r for r in rxns\n",
    "        if not r.startswith('EX_')\n",
    "        and r != 'BIOMASS_Ec_iAF1260_core_59p81M'\n",
    "        and r not in missing_set\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop empty groups\n",
    "Fully_Coupled_df = Fully_Coupled_df[Fully_Coupled_df['Coupled_Reactions'].str.len() > 0]\n",
    "\n",
    "# Report removals\n",
    "print(f\"Total 'EX_' reactions removed: {ex_removed_count}\")\n",
    "print(f\"'BIOMASS_Ec_iAF1260_core_59p81M' was removed: {biomass_removed_flag}\")\n",
    "print(f\"Total reactions removed due to fastcc removal: {missing_removed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster connectivity\n",
    "Fully_Coupled_df['Cluster'] = Fully_Coupled_df['Coupled_Reactions'].apply(\n",
    "    lambda rxn_list: are_reactions_interconnected(consistent_generic_model, rxn_list)\n",
    ")\n",
    "\n",
    "# Count number of reactions\n",
    "Fully_Coupled_df['Num_Reactions'] = Fully_Coupled_df['Coupled_Reactions'].apply(len)\n",
    "\n",
    "# Remove disconnected pairs of size 2\n",
    "Fully_Coupled_df = Fully_Coupled_df[~((Fully_Coupled_df['Num_Reactions'] == 2) & (Fully_Coupled_df['Cluster'] == False))]\n",
    "\n",
    "# Fix broken clusters with more than 2 reactions\n",
    "mask = (Fully_Coupled_df['Cluster'] == False) & (Fully_Coupled_df['Num_Reactions'] > 2)\n",
    "Fully_Coupled_df.loc[mask, 'Cleaned_Coupled_Reactions'] = Fully_Coupled_df.loc[mask, 'Coupled_Reactions'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(consistent_generic_model, rxns)\n",
    ")\n",
    "\n",
    "# Keep reactions as-is for connected groups\n",
    "Fully_Coupled_df['Cleaned_Coupled_Reactions'] = Fully_Coupled_df['Cleaned_Coupled_Reactions'].fillna(Fully_Coupled_df['Coupled_Reactions'])\n",
    "\n",
    "# Track removed reactions per group\n",
    "Fully_Coupled_df['Removed_Reactions'] = Fully_Coupled_df.apply(\n",
    "    lambda row: list(set(row['Coupled_Reactions']) - set(row['Cleaned_Coupled_Reactions'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final DataFrame for analysis\n",
    "selected_columns = ['Cleaned_Coupled_Reactions', 'Removed_Reactions']\n",
    "Lumping_Couples_df = Fully_Coupled_df[selected_columns].copy()\n",
    "\n",
    "# Drop single-reaction groups\n",
    "Lumping_Couples_df = Lumping_Couples_df[Lumping_Couples_df['Cleaned_Coupled_Reactions'].str.len() > 1]\n",
    "\n",
    "# Map reactions to subsystems\n",
    "subsystem_map = dict(zip(subsystem_df['iAF1260_BIGG'], subsystem_df['Subsystem']))\n",
    "Lumping_Couples_df['Subsystem'] = Lumping_Couples_df['Cleaned_Coupled_Reactions'].apply(\n",
    "    lambda rxn_list: list(set(subsystem_map.get(rxn, 'Unknown') for rxn in rxn_list))\n",
    ")\n",
    "\n",
    "# Add summary columns\n",
    "Lumping_Couples_df['Num_Subsystems'] = Lumping_Couples_df['Subsystem'].apply(len)\n",
    "Lumping_Couples_df['Num_Reactions'] = Lumping_Couples_df['Cleaned_Coupled_Reactions'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Lump fully coupled clusters into pseudo-reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lumped, translation_df = lump_reaction(consistent_generic_model, Lumping_Couples_df, reaction_column=\"Cleaned_Coupled_Reactions\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Removed 0 internal metabolites used only in pseudo-reactions.\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove orphaned metabolites used only in pseudo-reactions\n",
    "metabolites_to_remove = []\n",
    "\n",
    "for met in model_lumped.metabolites:\n",
    "    # Get all reactions involving this metabolite\n",
    "    associated_rxns = [rxn.id for rxn in met.reactions]\n",
    "    \n",
    "    # If the metabolite only appears in one reaction AND it's a pseudo-reaction\n",
    "    if len(associated_rxns) == 1 and associated_rxns[0].startswith(\"Pseudo_\"):\n",
    "        metabolites_to_remove.append(met)\n",
    "\n",
    "# Remove them from the model\n",
    "model_lumped.remove_metabolites(metabolites_to_remove)\n",
    "\n",
    "print(f\"ðŸ§¹ Removed {len(metabolites_to_remove)} internal metabolites used only in pseudo-reactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 887\n",
      "Number of Reactions: 1545\n",
      "Degrees of Freedom: 658\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(model_lumped)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(model_lumped.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model FBA objective: 36.400622\n",
      "Lumped model FBA objective:   36.400622\n",
      "âœ… FBA optimum preserved.\n"
     ]
    }
   ],
   "source": [
    "sol_orig = model_copy.optimize()\n",
    "sol_lumped = model_lumped.optimize()\n",
    "\n",
    "print(f\"Original model FBA objective: {sol_orig.objective_value:.6f}\")\n",
    "print(f\"Lumped model FBA objective:   {sol_lumped.objective_value:.6f}\")\n",
    "\n",
    "if abs(sol_orig.objective_value - sol_lumped.objective_value) < 1e-6:\n",
    "    print(\"âœ… FBA optimum preserved.\")\n",
    "else:\n",
    "    print(\"âš ï¸ FBA optimum changed â€” check lumping effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pseudo-reactions: 347\n",
      "Blocked pseudo-reactions: 0\n",
      "âœ… All pseudo-reactions are feasible.\n"
     ]
    }
   ],
   "source": [
    "# Get only the pseudo-reactions\n",
    "pseudo_rxns = [rxn.id for rxn in model_lumped.reactions if rxn.id.startswith(\"Pseudo_\")]\n",
    "\n",
    "# Run FVA on those reactions\n",
    "fva_result = flux_variability_analysis(model_lumped, reaction_list=pseudo_rxns, fraction_of_optimum=0.0)\n",
    "\n",
    "# Check which pseudo-reactions are blocked (min=max=0)\n",
    "blocked = fva_result[(fva_result[\"minimum\"] == 0) & (fva_result[\"maximum\"] == 0)]\n",
    "\n",
    "print(f\"Total pseudo-reactions: {len(pseudo_rxns)}\")\n",
    "print(f\"Blocked pseudo-reactions: {len(blocked)}\")\n",
    "\n",
    "if not blocked.empty:\n",
    "    print(\"âš ï¸ Some pseudo-reactions are blocked:\")\n",
    "    print(blocked)\n",
    "else:\n",
    "    print(\"âœ… All pseudo-reactions are feasible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsystem based lumping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Group by module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with subsystem mapping\n",
    "subsystem_map = dict(zip(subsystem_df['iAF1260_BIGG'], subsystem_df['Subsystem']))\n",
    "\n",
    "# Create working copy\n",
    "translation_cp_df = translation_df.copy()\n",
    "\n",
    "# Apply the logic inline, returning a tuple (Subsystem, Unambiguous_Subsystem), then split into columns\n",
    "translation_cp_df[['Subsystem', 'Unambiguous_Subsystem']] = translation_cp_df['Original_Reactions'].apply(\n",
    "    lambda reactions: (\n",
    "        (lambda subsystems: (\n",
    "            Counter(subsystems).most_common(1)[0][0] if subsystems else None,\n",
    "            True if len(set(Counter(subsystems).values())) == 1 and len(set(subsystems)) == 1 else False\n",
    "        ))([subsystem_map[rxn] for rxn in reactions if rxn in subsystem_map])\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "\n",
    "# Create new rows from subsystem_df\n",
    "new_rows = pd.DataFrame({\n",
    "    'Pseudo_Reaction_ID': subsystem_df['iAF1260_BIGG'],\n",
    "    'Original_Reactions': [None] * len(subsystem_df),\n",
    "    'Subsystem': subsystem_df['Subsystem'],\n",
    "    'Unambiguous_Subsystem': [True] * len(subsystem_df)  # Set to True explicitly\n",
    "})\n",
    "\n",
    "# Append new rows to the main DataFrame\n",
    "translation_cp_df = pd.concat([translation_cp_df, new_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract all reaction IDs from model_lumped\n",
    "model_reaction_ids = [rxn.id for rxn in model_lumped.reactions]\n",
    "\n",
    "# Subset translation_df to only those rows with matching Pseudo_Reaction_ID\n",
    "subset_df = translation_cp_df[translation_cp_df['Pseudo_Reaction_ID'].isin(model_reaction_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Subsystem' and aggregate\n",
    "grouped_df = subset_df.groupby('Subsystem').agg(\n",
    "    Members=('Pseudo_Reaction_ID', list),\n",
    "    Member_Count=('Pseudo_Reaction_ID', 'count')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Remove modules that cannot be lumped\n",
    "\n",
    "Note: For Core, reference: *Ataman, et al. \"redGEM: Systematic reduction ... consistent core metabolic models.\" PLoS computational biology 13.7 (2017): e1005444.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forbidden_subsystems_list = ['S_Exchange','S_Unassigned','S_']\n",
    "\n",
    "Core_subsystems_list = ['S_GlycolysisGluconeogenesis','S_Pentose_Phosphate_Pathway','S_Citric_Acid_Cycle','S_Glyoxylate_Metabolism','S_Pyruvate_Metabolism','S_Oxidative_Phosphorylation']\n",
    "\n",
    "Remove_list = Forbidden_subsystems_list + Core_subsystems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset: rows to remove (core)\n",
    "Core_grouped_df = grouped_df[grouped_df['Subsystem'].isin(Remove_list)].copy()\n",
    "\n",
    "# Subset: rows to keep (non-core)\n",
    "Noncore_grouped_df = grouped_df[~grouped_df['Subsystem'].isin(Remove_list)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Connect to relevance for biomass reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metabolites required by the biomass reaction (educts)\n",
    "biomass_rxn = model_lumped.reactions.get_by_id(\"BIOMASS_Ec_iAF1260_core_59p81M\")\n",
    "biomass_educts = {met.id for met, coeff in biomass_rxn.metabolites.items() if coeff < 0}\n",
    "\n",
    "# Containers for new columns\n",
    "educts_list = []\n",
    "products_list = []\n",
    "internal_list = []\n",
    "biomass_outputs = []\n",
    "biomass_flags = []\n",
    "\n",
    "# Iterate over each subsystem group (each row)\n",
    "for _, row in Noncore_grouped_df.iterrows():\n",
    "    reaction_ids = row['Members']\n",
    "    stoich = defaultdict(float)\n",
    "\n",
    "    # Accumulate net stoichiometry across all reactions\n",
    "    for rxn_id in reaction_ids:\n",
    "        if rxn_id not in model_lumped.reactions:\n",
    "            continue\n",
    "        rxn = model_lumped.reactions.get_by_id(rxn_id)\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            stoich[met.id] += coeff\n",
    "\n",
    "    # Classify metabolites\n",
    "    educt_met = [m for m, c in stoich.items() if c < 0]\n",
    "    product_met = [m for m, c in stoich.items() if c > 0]\n",
    "\n",
    "    # Biomass-relevant products\n",
    "    biomass_met = [m for m in product_met if m in biomass_educts]\n",
    "    biomass_relevant = len(biomass_met) > 0\n",
    "\n",
    "    # Append results\n",
    "    educts_list.append(educt_met)\n",
    "    products_list.append(product_met)\n",
    "    biomass_outputs.append(biomass_met)\n",
    "    biomass_flags.append(biomass_relevant)\n",
    "\n",
    "# Assign new columns to grouped_df\n",
    "Noncore_grouped_df['educt_met'] = educts_list\n",
    "Noncore_grouped_df['product_met'] = products_list\n",
    "Noncore_grouped_df['biomass'] = biomass_outputs\n",
    "Noncore_grouped_df['biomass_relevant'] = biomass_flags\n",
    "\n",
    "Noncore_grouped_df['Interconnected'] = Noncore_grouped_df['Members'].apply(\n",
    "    lambda rxns: are_reactions_interconnected(model_lumped, rxns)\n",
    ")\n",
    "\n",
    "Noncore_grouped_df.sort_values(by='Member_Count',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find largest subclusters that are interconnected for non-interconnected groups\n",
    "mask = (Noncore_grouped_df['Interconnected'] == False)\n",
    "\n",
    "Noncore_grouped_df.loc[mask, 'Members_Cleaned_Reactions'] = Noncore_grouped_df.loc[mask, 'Members'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(model_lumped, rxns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df['Members_Cleaned_Reactions_Number'] = Noncore_grouped_df['Members_Cleaned_Reactions'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df[\"Leftover_Members\"] = Noncore_grouped_df.apply(\n",
    "    lambda row: list(set(row[\"Members\"]) - set(row[\"Members_Cleaned_Reactions\"]))\n",
    "    if isinstance(row[\"Members_Cleaned_Reactions\"], list) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find second largest subclusters that are interconnected for non-interconnected groups\n",
    "\n",
    "mask = Noncore_grouped_df[\"Members_Cleaned_Reactions\"].notna()\n",
    "\n",
    "# Apply function only to rows where the mask is True\n",
    "Noncore_grouped_df.loc[mask, 'Leftover_Cleaned_Members'] = Noncore_grouped_df.loc[mask, 'Leftover_Members'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(model_lumped, rxns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df['Leftover_Cleaned_Members_Number'] = Noncore_grouped_df['Leftover_Cleaned_Members'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lump Subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Truly interconnected subsystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to initially true ones\n",
    "copy1_df = Noncore_grouped_df[Noncore_grouped_df['Interconnected'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy1, translation_df = lump_reaction(model_lumped, copy1_df, reaction_column=\"Members\",verbose=False,label_type=\"Subsystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 797\n",
      "Number of Reactions: 1284\n",
      "Degrees of Freedom: 487\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(copy1)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(copy1.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Largest Subclusters of non interconnected Subsystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2_df = Noncore_grouped_df[Noncore_grouped_df[\"Members_Cleaned_Reactions\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2, translation_df = lump_reaction(copy1, copy2_df, reaction_column=\"Members_Cleaned_Reactions\",verbose=False,label_type=\"Subsystem_Cluster1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 479\n",
      "Number of Reactions: 540\n",
      "Degrees of Freedom: 61\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(copy2)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(copy2.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: can get to degree 34 when linking all other reactions of clusters, but then destroying network stucture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Specifity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model = copy.deepcopy(copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all exchange reactions\n",
    "exchange_reactions = context_model.exchanges  # model.exchanges returns a list of exchange reactions\n",
    "\n",
    "# Build the dataframe\n",
    "exchange_df = pd.DataFrame({\n",
    "    \"ID\": [rxn.id for rxn in exchange_reactions],\n",
    "    \"Name\": [rxn.name for rxn in exchange_reactions]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all exchange reactions in model1\n",
    "for rxn in model.exchanges:\n",
    "    rxn_id = rxn.id\n",
    "\n",
    "    # Check if the same exchange reaction exists in model2\n",
    "    if rxn_id in context_model.reactions:\n",
    "        # Get the corresponding reaction in model2\n",
    "        target_rxn = context_model.reactions.get_by_id(rxn_id)\n",
    "\n",
    "        # Copy bounds\n",
    "        target_rxn.lower_bound = rxn.lower_bound\n",
    "        target_rxn.upper_bound = rxn.upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Paths for Biomass Precursors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Extract Biomass Precursors, Core Reactions & Metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reactants (educts) of the biomass reaction\n",
    "biomass_rxn = model_lumped.reactions.get_by_id(\"BIOMASS_Ec_iAF1260_core_59p81M\")\n",
    "biomass_educts = [met.id for met in biomass_rxn.reactants]\n",
    "original_stoich = biomass_rxn.metabolites  # dict: {met: coeff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all Core reactions & Core metabolites\n",
    "\n",
    "# Combine all reaction IDs from the 'core' subsystems in Remove_list\n",
    "core_reactions = set()\n",
    "\n",
    "# Filter rows where 'Subsystem' is in Remove_list\n",
    "filtered_df = grouped_df[grouped_df['Subsystem'].isin(Remove_list)]\n",
    "\n",
    "# Loop through those rows and collect reactions\n",
    "for members in filtered_df['Members']:\n",
    "    core_reactions.update(members)  # Assuming each entry is a list of reaction IDs\n",
    "\n",
    "# Extract all metabolites from Core reactions\n",
    "core_metabolites = set()\n",
    "for rxn_id in core_reactions:\n",
    "    try:\n",
    "        rxn = model_lumped.reactions.get_by_id(rxn_id)\n",
    "        for met in rxn.metabolites:\n",
    "            core_metabolites.add(met.id)\n",
    "    except KeyError:\n",
    "        print(f\"Reaction {rxn_id} not found in model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Missing from core_metabolites: []\n"
     ]
    }
   ],
   "source": [
    "# Check if all metabolites are in core subset\n",
    "\n",
    "intersected = list(set(core_metabolites) & set(biomass_educts))\n",
    "\n",
    "all_in_core = set(biomass_educts).issubset(set(core_metabolites))\n",
    "print(all_in_core)\n",
    "\n",
    "# If False, can print missing ones!\n",
    "\n",
    "missing = [met for met in biomass_educts if met not in core_metabolites]\n",
    "print(\"Missing from core_metabolites:\", missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Get minimum core precursors for each biomass precursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 4 precursors not reachable â†’ ['ca2_c', 'cobalt2_c', 'cu2_c', 'murein5px4p_p']\n",
      "Step 2: 1 precursors not reachable â†’ ['murein5px4p_p']\n",
      "Step 3: 0 precursors not reachable â†’ []\n"
     ]
    }
   ],
   "source": [
    "# Determine at what depth each biomass precursor becomes reachable\n",
    "min_depth_map = {met: 1 for met in biomass_educts}  # default to 1\n",
    "\n",
    "unreachable_by_step = report_unreachable_precursors(model_lumped,biomass_educts,core_metabolites,max_depth_range=3)\n",
    "\n",
    "for depth, unreachable in unreachable_by_step.items():\n",
    "    for met in unreachable:\n",
    "        min_depth_map[met] = depth + 1  # It's not reachable at this depth, so try next # Run find_precursors for each individual biomass precursor at its required depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Create Summary for Pseudo-Reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "# For each biomass precursor, use the minimum depth needed\n",
    "for biomass_met in biomass_educts:\n",
    "    required_depth = min_depth_map[biomass_met]\n",
    "\n",
    "    result = find_precursors(model_lumped, [biomass_met], core_metabolites, max_depth=required_depth)\n",
    "    precursors = result.get(biomass_met, set())\n",
    "\n",
    "    records.append({\n",
    "        'biomass_precursor': biomass_met,\n",
    "        'core_precursors': sorted(list(precursors)),\n",
    "        'k_steps': required_depth\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "df_precursor_summary = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove mappings for biomass precursors that are in fact core metabolites \n",
    "\n",
    "# Unique biomass_precursor values\n",
    "biomass_set = set(df_precursor_summary['biomass_precursor'])\n",
    "\n",
    "# Flatten all lists in 'core_precursors' column into one set\n",
    "core_precursor_set = set()\n",
    "for precursors in df_precursor_summary['core_precursors']:\n",
    "    core_precursor_set.update(precursors)\n",
    "\n",
    "# Find intersection\n",
    "core_and_biomass_precursors = sorted(biomass_set & core_precursor_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the DataFrame\n",
    "df_precursor_summary_filtered = df_precursor_summary[~df_precursor_summary['biomass_precursor'].isin(core_and_biomass_precursors)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Create Pseudo Reactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a deep copy of the model to avoid modifying the original\n",
    "model_biomass = copy.deepcopy(model_lumped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_reactions = []\n",
    "\n",
    "for _, row in df_precursor_summary.iterrows():\n",
    "    biomass_met_id = row['biomass_precursor']\n",
    "    core_precursor_ids = row['core_precursors']\n",
    "\n",
    "    # Create new Reaction object\n",
    "    rxn = Reaction(id=f\"Pseudo_Biomass_{biomass_met_id}\")\n",
    "    rxn.name = f\"Pseudo_Biomass_{biomass_met_id}\"\n",
    "    rxn.lower_bound = 0\n",
    "    rxn.upper_bound = 1000\n",
    "\n",
    "    # Build stoichiometry dict: reactants have negative coeffs, product is positive\n",
    "    stoich = {}\n",
    "\n",
    "    for met_id in core_precursor_ids:\n",
    "        met = model_biomass.metabolites.get_by_id(met_id)\n",
    "        stoich[met] = -1.0  # default stoichiometry\n",
    "\n",
    "    biomass_met = model_biomass.metabolites.get_by_id(biomass_met_id)\n",
    "    stoich[biomass_met] = 1.0  # product\n",
    "\n",
    "    # Add metabolites and append to list\n",
    "    rxn.add_metabolites(stoich)\n",
    "    pseudo_reactions.append(rxn)\n",
    "\n",
    "# Add all pseudo-reactions to the model\n",
    "model_biomass.add_reactions(pseudo_reactions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cobra_escher_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
