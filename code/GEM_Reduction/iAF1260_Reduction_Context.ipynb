{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library and 3rd Party Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import copy\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COBRA imports\n",
    "\n",
    "import cobra\n",
    "from cobra import Model, Reaction\n",
    "from cobra.core import Group, Reaction\n",
    "from cobra.flux_analysis import flux_variability_analysis\n",
    "from cobra.flux_analysis.fastcc import fastcc\n",
    "from cobra.io import read_sbml_model, save_matlab_model, write_sbml_model\n",
    "from cobra.util.solver import linear_reaction_coefficients\n",
    "from rapidfuzz import fuzz, process\n",
    "from networkx.algorithms import bipartite\n",
    "from scipy.linalg import svd\n",
    "from cobra.util.array import create_stoichiometric_matrix\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_reactions_interconnected(model, reaction_ids):\n",
    "    \"\"\"\n",
    "    Check whether a list of reactions are all interconnected via shared metabolites.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "    - reaction_ids: list of reaction IDs to test\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if all reactions are connected via shared metabolites\n",
    "    \"\"\"\n",
    "    # Create a graph where nodes = reactions, edges = shared metabolite\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(reaction_ids)\n",
    "\n",
    "    # Build edges based on shared metabolites\n",
    "    for i, rxn1_id in enumerate(reaction_ids):\n",
    "        if rxn1_id not in model.reactions:\n",
    "            continue\n",
    "        rxn1 = model.reactions.get_by_id(rxn1_id)\n",
    "        mets1 = set(rxn1.metabolites)\n",
    "\n",
    "        for rxn2_id in reaction_ids[i+1:]:\n",
    "            if rxn2_id not in model.reactions:\n",
    "                continue\n",
    "            rxn2 = model.reactions.get_by_id(rxn2_id)\n",
    "            mets2 = set(rxn2.metabolites)\n",
    "\n",
    "            # Add edge if they share at least one metabolite\n",
    "            if mets1 & mets2:\n",
    "                G.add_edge(rxn1_id, rxn2_id)\n",
    "\n",
    "    # Check if the graph is fully connected\n",
    "    return nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_connected_subgroup(model, reaction_list):\n",
    "    \"\"\"\n",
    "    Identify the largest connected component of a group of reactions based on shared metabolites.\n",
    "\n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "        A COBRA model containing reactions and metabolites.\n",
    "    - reaction_list: list of str\n",
    "        A list of reaction IDs to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - list of str:\n",
    "        The subset of reaction IDs from the input that belong to the largest connected component.\n",
    "        Connectivity is defined by reactions sharing at least one metabolite (bipartite graph: reactions ↔ metabolites).\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for rxn_id in reaction_list:\n",
    "        rxn = model.reactions.get_by_id(rxn_id)\n",
    "        for met in rxn.metabolites:\n",
    "            G.add_edge(rxn.id, met.id)\n",
    "\n",
    "    components = nx.connected_components(G)\n",
    "    \n",
    "    reaction_components = [\n",
    "        set(comp).intersection(reaction_list) for comp in components\n",
    "    ]\n",
    "    \n",
    "    largest = max(reaction_components, key=len, default=[])\n",
    "    return list(largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lump_reaction(model, coupled_df, reaction_column=\"Coupled_Reactions\", verbose=True, label_type=\"Group\"):\n",
    "    \"\"\"\n",
    "    Create a lumped model by replacing fully coupled reaction groups with pseudo-reactions.\n",
    "\n",
    "    Parameters:\n",
    "    - model: cobra.Model\n",
    "        The original COBRA model (remains unchanged).\n",
    "    - coupled_df: pd.DataFrame\n",
    "        DataFrame with a column containing lists of reactions to lump.\n",
    "    - reaction_column: str\n",
    "        The column name in `coupled_df` that holds lists of reactions (default: \"Coupled_Reactions\").\n",
    "    - verbose: bool\n",
    "        If True, print progress messages.\n",
    "    - label_type: str\n",
    "        Label prefix for naming the pseudo-reactions (e.g., \"Group\", \"Module\").\n",
    "\n",
    "    Returns:\n",
    "    - model_lumped: cobra.Model\n",
    "        A copy of the model with pseudo-reactions added and original reactions removed.\n",
    "    - translation_df: pd.DataFrame\n",
    "        DataFrame mapping pseudo-reactions to their original reaction IDs.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    from cobra import Reaction\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "\n",
    "    model_lumped = copy.deepcopy(model)\n",
    "    translation_data = []\n",
    "\n",
    "    for i, row in coupled_df.iterrows():\n",
    "        group_reactions = row[reaction_column]\n",
    "\n",
    "        # Filter only reactions that exist in the model\n",
    "        valid_reactions = [rxn_id for rxn_id in group_reactions if rxn_id in model_lumped.reactions]\n",
    "\n",
    "        if len(valid_reactions) < 2:\n",
    "            if verbose:\n",
    "                print(f\"⏭️ Skipping group {i} — fewer than 2 valid reactions.\")\n",
    "            continue\n",
    "\n",
    "        # Determine common flux bounds\n",
    "        lower_bounds = [model_lumped.reactions.get_by_id(r).lower_bound for r in valid_reactions]\n",
    "        upper_bounds = [model_lumped.reactions.get_by_id(r).upper_bound for r in valid_reactions]\n",
    "        min_lb = max(lower_bounds)\n",
    "        max_ub = min(upper_bounds)\n",
    "\n",
    "        # Compute net stoichiometry\n",
    "        net_stoich = defaultdict(float)\n",
    "        for rxn_id in valid_reactions:\n",
    "            rxn = model_lumped.reactions.get_by_id(rxn_id)\n",
    "            for met, coeff in rxn.metabolites.items():\n",
    "                net_stoich[met] += coeff\n",
    "        cleaned_stoich = {met: coeff for met, coeff in net_stoich.items() if abs(coeff) > 1e-10}\n",
    "\n",
    "        # Create pseudo-reaction\n",
    "        pseudo_id = f\"Pseudo_{label_type}_{i}\"\n",
    "        pseudo_rxn = Reaction(id=pseudo_id)\n",
    "        pseudo_rxn.name = f\"Lumped reaction for: {', '.join(valid_reactions)}\"\n",
    "        pseudo_rxn.lower_bound = min_lb\n",
    "        pseudo_rxn.upper_bound = max_ub\n",
    "        pseudo_rxn.add_metabolites(cleaned_stoich)\n",
    "\n",
    "        # Add and remove reactions\n",
    "        model_lumped.add_reactions([pseudo_rxn])\n",
    "        for rxn_id in valid_reactions:\n",
    "            model_lumped.reactions.remove(model_lumped.reactions.get_by_id(rxn_id))\n",
    "\n",
    "        # Record mapping\n",
    "        translation_data.append({\n",
    "            'Pseudo_Reaction_ID': pseudo_id,\n",
    "            'Original_Reactions': valid_reactions\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"✅ {label_type} {i}: Created '{pseudo_id}' from {valid_reactions}\")\n",
    "\n",
    "    translation_df = pd.DataFrame(translation_data)\n",
    "    return model_lumped, translation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_precursors(model, biomass_precursors, core_mets, max_depth=5):\n",
    "    \"\"\"\n",
    "    Find core metabolites that can act as precursors to each biomass precursor,\n",
    "    using reactions in the model. Handles reversibility and stoichiometry.\n",
    "\n",
    "    Parameters:\n",
    "        model (cobra.Model): The metabolic model.\n",
    "        biomass_precursors (list): List of metabolite IDs in the biomass reaction.\n",
    "        core_mets (list): List of core metabolite IDs.\n",
    "        max_depth (int): Maximum number of reactions to search backwards.\n",
    "\n",
    "    Returns:\n",
    "        dict: {biomass_precursor: set of core_metabolites_that_can_reach_it}\n",
    "    \"\"\"\n",
    "    # Build reverse graph: met_id -> reactions that can produce it\n",
    "    met_to_producing_rxns = defaultdict(list)\n",
    "\n",
    "    for rxn in model.reactions:\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            if coeff > 0:\n",
    "                # Metabolite is produced in forward direction\n",
    "                met_to_producing_rxns[met.id].append((rxn, 'forward'))\n",
    "            elif coeff < 0 and rxn.lower_bound < 0:\n",
    "                # Metabolite is produced in reverse direction\n",
    "                met_to_producing_rxns[met.id].append((rxn, 'reverse'))\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for target_met in biomass_precursors:\n",
    "        visited_mets = set()\n",
    "        visited_rxns = set()\n",
    "        queue = deque([(target_met, 0)])  # (metabolite_id, depth)\n",
    "        precursors = set()\n",
    "\n",
    "        while queue:\n",
    "            current_met, depth = queue.popleft()\n",
    "            if depth >= max_depth:\n",
    "                continue\n",
    "\n",
    "            producing_rxns = met_to_producing_rxns.get(current_met, [])\n",
    "\n",
    "            for rxn, direction in producing_rxns:\n",
    "                if (rxn.id, direction) in visited_rxns:\n",
    "                    continue\n",
    "                visited_rxns.add((rxn.id, direction))\n",
    "\n",
    "                # Get reactants based on direction\n",
    "                if direction == 'forward':\n",
    "                    reactants = rxn.reactants\n",
    "                else:\n",
    "                    reactants = rxn.products  # in reverse, products become precursors\n",
    "\n",
    "                for reactant in reactants:\n",
    "                    if reactant.id in visited_mets:\n",
    "                        continue\n",
    "                    visited_mets.add(reactant.id)\n",
    "\n",
    "                    if reactant.id in core_mets:\n",
    "                        precursors.add(reactant.id)\n",
    "                    else:\n",
    "                        queue.append((reactant.id, depth + 1))\n",
    "\n",
    "        result[target_met] = precursors\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model, model_name, carb_list):\n",
    "    \"\"\"\n",
    "    Run FBA simulations for a COBRA model across multiple carbon source conditions.\n",
    "\n",
    "    For each carbon source in the provided list, this function:\n",
    "    - Allows uptake only for that carbon source (by setting its flux bounds).\n",
    "    - Blocks all other carbon sources.\n",
    "    - Runs Flux Balance Analysis (FBA).\n",
    "    - Records which reactions are active (i.e., carry non-zero flux).\n",
    "    - Stores the model's objective value (e.g., growth rate).\n",
    "\n",
    "    The function returns:\n",
    "    1. A summary DataFrame with objective values for each carbon source.\n",
    "    2. A full flux activity matrix (True/False for each reaction in each condition).\n",
    "    3. A filtered version of the matrix showing only reactions that are active in some but not all conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : cobra.Model\n",
    "        The COBRA model to simulate.\n",
    "\n",
    "    model_name : str\n",
    "        Identifier for the model (used in the output DataFrame).\n",
    "\n",
    "    carb_list : list of str\n",
    "        List of exchange reaction IDs corresponding to carbon sources to be tested.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame with columns ['Model', 'Carbon_Source', 'Objective_Value'].\n",
    "\n",
    "    flux_activity_df : pandas.DataFrame\n",
    "        Boolean DataFrame with reactions as rows and carbon sources as columns.\n",
    "        True indicates the reaction carried flux in that condition.\n",
    "\n",
    "    filtered_flux_activity_df : pandas.DataFrame\n",
    "        Subset of `flux_activity_df` containing only reactions that are active\n",
    "        in some but not all carbon source conditions.\n",
    "    \"\"\"\n",
    "    flux_threshold = 1e-6  # Minimum threshold to consider flux as active\n",
    "\n",
    "    results = []\n",
    "    flux_activity_df = pd.DataFrame({'Reaction': [rxn.id for rxn in model.reactions]})\n",
    "    flux_activity_df.set_index('Reaction', inplace=True)\n",
    "\n",
    "    for carb in carb_list:\n",
    "        model_temp = model.copy()\n",
    "\n",
    "        # Block all carbon sources except the current one\n",
    "        for rxn_id in carb_list:\n",
    "            if rxn_id in model_temp.reactions:\n",
    "                rxn = model_temp.reactions.get_by_id(rxn_id)\n",
    "                if rxn_id == carb:\n",
    "                    rxn.lower_bound = -10.0\n",
    "                    rxn.upper_bound = 1000.0\n",
    "                else:\n",
    "                    rxn.lower_bound = 0.0\n",
    "                    rxn.upper_bound = 0.0\n",
    "\n",
    "        # Run FBA\n",
    "        solution = model_temp.optimize()\n",
    "\n",
    "        # Record objective value\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Carbon_Source': carb,\n",
    "            'Objective_Value': solution.objective_value\n",
    "        })\n",
    "\n",
    "        # Record which reactions carry flux\n",
    "        active_flux = solution.fluxes.abs() > flux_threshold\n",
    "        flux_activity_df[carb] = active_flux\n",
    "\n",
    "    # Filter reactions active in some but not all conditions\n",
    "    filtered_flux_activity_df = flux_activity_df[\n",
    "        ~(flux_activity_df.all(axis=1) | ~flux_activity_df.any(axis=1))\n",
    "    ]\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, flux_activity_df, filtered_flux_activity_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project root path by going up from the notebook location\n",
    "notebook_dir = Path(__file__).parent if '__file__' in globals() else Path().resolve()\n",
    "project_root = notebook_dir.parent.parent  # Go up from /code/GEM_Reduction/\n",
    "\n",
    "# Construct raw paths \n",
    "raw_data_path = project_root / \"data\" / \"raw\" \n",
    "raw_sbml_path = raw_data_path / \"sbml_files\" \n",
    "raw_mat_path = raw_data_path / \"matlab_files\" \n",
    "raw_csv_path = raw_data_path / \"csv_files\" \n",
    "\n",
    "# Construct processed paths \n",
    "processed_data_path = project_root / \"data\" / \"processed\" \n",
    "processed_sbml_path = processed_data_path / \"sbml_files\" \n",
    "processed_csv_path = processed_data_path / \"csv_files\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model\n",
    "model = read_sbml_model(raw_sbml_path / \"iAF1260.xml\")\n",
    "subsystem_df = pd.read_csv(raw_csv_path / 'iAF1260_subsystem_assignments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a curated list of exchange reactions from file\n",
    "exchange_df_full = pd.read_csv(raw_csv_path / 'iAF1260_exchanges.csv')\n",
    "final_exchanges_df = pd.read_csv(raw_csv_path / 'iAF1260_context_exchanges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pairs coupled and reactions blocked by F2C2\n",
    "Coupled_Pairs_df = pd.read_csv(raw_csv_path / 'fctable_iAF1260_context.csv', header=None)\n",
    "F2C2_Blocked_Reactions_df = pd.read_csv(raw_csv_path / 'blocked_reactions_iAF1260_context.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Basics\n",
    "- Model ID: iAF1260\n",
    "- Number of Reactions: 2382\n",
    "- Number of Metabolites: 1668\n",
    "- Number of Genes: 1261\n",
    "- Number of Exchange Reactions: 299\n",
    "- Reversible Reactions: 575\n",
    "- Irreversible Reactions: 1807\n",
    "- Objective reaction(s): R_BIOMASS_Ec_iAF1260_core_59p81M\n",
    "\n",
    "- FBA Objective value (biomass): 0.7367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 1630\n",
      "Number of Reactions: 2382\n",
      "Degrees of Freedom: 752\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(model)  # Already a NumPy array\n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(model.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a deep copy of the model to avoid modifying the original\n",
    "model_copy = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Create Files to invest exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run FBA on the model\n",
    "solution = model_copy.optimize()\n",
    "\n",
    "# Step 2: Extract all exchange reactions from the model\n",
    "exchange_reactions = model_copy.exchanges\n",
    "\n",
    "# Step 3: Create a DataFrame summarizing each exchange reaction\n",
    "exchange_df = pd.DataFrame({\n",
    "    \"ID\": [rxn.id for rxn in exchange_reactions],\n",
    "    \"Name\": [rxn.name for rxn in exchange_reactions],\n",
    "    \"Lower_FB\": [rxn.lower_bound for rxn in exchange_reactions],\n",
    "    \"Upper_FB\": [rxn.upper_bound for rxn in exchange_reactions],\n",
    "})\n",
    "\n",
    "# Step 4: Add the FBA flux value for each exchange reaction\n",
    "exchange_df[\"Flux\"] = exchange_df[\"ID\"].apply(lambda rxn_id: solution.fluxes.get(rxn_id, 0.0))\n",
    "\n",
    "# Step 5: Identify which reactions are active (i.e., flux is significantly non-zero)\n",
    "exchange_df[\"Active_in_FBA\"] = exchange_df[\"Flux\"].apply(lambda v: abs(v) > 1e-6)\n",
    "\n",
    "# (Optional) Step 6: Duplicate 'Active_in_FBA' column (may be removed if not needed)\n",
    "exchange_df[\"Active_in_FBA_Copy\"] = exchange_df[\"Active_in_FBA\"]\n",
    "\n",
    "# Step 7: Filter to only active exchange reactions and create a copy\n",
    "exchange_df_sub = exchange_df[exchange_df[\"Active_in_FBA_Copy\"] == True].copy()\n",
    "\n",
    "# Step 8: Add 'Import' column:\n",
    "#   - True if flux < 0 (uptake)\n",
    "#   - False if flux > 0 (secretion)\n",
    "#   - NaN if zero flux (shouldn't appear here due to filter)\n",
    "exchange_df_sub[\"Import\"] = exchange_df_sub[\"Flux\"].apply(\n",
    "    lambda v: True if v < 0 else (False if v > 0 else pd.NA)\n",
    ")\n",
    "\n",
    "# Step 10: Filter the list to those reactions marked for inclusion\n",
    "exchange_df2 = exchange_df_full[exchange_df_full[\"Include\"] == True]\n",
    "\n",
    "# Step 11: Merge the curated exchange list (exchange_df2) with activity/Import info from FBA\n",
    "# Left join ensures all included reactions are preserved; Import info added where available\n",
    "merged_df = exchange_df2.merge(\n",
    "    exchange_df_sub[[\"ID\", \"Import\"]],\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Import modified exchange reactions incl directionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a deep copy of the model to avoid modifying the original\n",
    "model_copy2 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IDs from final_exchanges_df\n",
    "allowed_ids = set(final_exchanges_df[\"ID\"])\n",
    "\n",
    "# Step 1: Block all exchange reactions NOT in the list\n",
    "for rxn in model_copy2.exchanges:\n",
    "    if rxn.id not in allowed_ids:\n",
    "        rxn.lower_bound = 0.0\n",
    "        rxn.upper_bound = 0.0\n",
    "\n",
    "# Step 2 and 3: Handle Import = False and Import = True separately\n",
    "for _, row in final_exchanges_df.iterrows():\n",
    "    rxn_id = row[\"ID\"]\n",
    "    is_import = row[\"Import\"]\n",
    "\n",
    "    # Get the reaction from the model\n",
    "    rxn = model_copy2.reactions.get_by_id(rxn_id)\n",
    "\n",
    "    if is_import is False:\n",
    "        # Export reaction: restrict lower bound to 0, keep upper as-is\n",
    "        rxn.lower_bound = 0.0\n",
    "\n",
    "    elif is_import is True:\n",
    "        # Import reaction: reverse it and adjust bounds\n",
    "\n",
    "        # Store previous lower bound before changing it\n",
    "        prev_lower = rxn.lower_bound\n",
    "\n",
    "        # Create reversed reaction\n",
    "        reversed_rxn = Reaction(id=f\"{rxn.id}_reversed\")\n",
    "        reversed_rxn.name = f\"Reversed {rxn.name}\"\n",
    "\n",
    "        # Reverse the stoichiometry (reactants <-> products)\n",
    "        reversed_rxn.add_metabolites({met: -coef for met, coef in rxn.metabolites.items()})\n",
    "\n",
    "        # Set bounds: lower = 0, upper = abs(previous lower)\n",
    "        reversed_rxn.lower_bound = 0.0\n",
    "        reversed_rxn.upper_bound = abs(prev_lower)\n",
    "\n",
    "        # Add the new reaction to the model and remove the old one\n",
    "        model_copy2.add_reactions([reversed_rxn])\n",
    "        model_copy2.remove_reactions([rxn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of carbon source exchange reactions\n",
    "carb_list = ['EX_glc__D_e_reversed', 'EX_fru_e_reversed', 'EX_sucr_e_reversed', 'EX_arab__L_e_reversed', 'EX_mnl_e_reversed']\n",
    "\n",
    "# Set bounds for each reaction\n",
    "for rxn_id in carb_list:\n",
    "    if rxn_id in model_copy2.reactions:\n",
    "        rxn = model_copy2.reactions.get_by_id(rxn_id)\n",
    "        rxn.lower_bound = 0.0\n",
    "        rxn.upper_bound = 2.0\n",
    "    else:\n",
    "        print(f\"Reaction {rxn_id} not found in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export as .mat for matlab applications\n",
    "\n",
    "save_matlab_model(model_copy2, raw_mat_path / \"iAF1260_context.mat\") # use generic model to get unbiased couplings (filter later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Run FASTCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce model with COBRApy FASTCC \n",
    "consistent_generic_model = fastcc(model_copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 861\n",
      "Number of Reactions: 1313\n",
      "Degrees of Freedom: 452\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(consistent_generic_model)  # Already a NumPy array\n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(consistent_generic_model.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1**: Make F2C2 Data accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reaction annotations to match F2C2 blocked reactions\n",
    "model_rxns = [rxn.id for rxn in model_copy2.reactions]\n",
    "F2C2_Blocked_Reactions_df.loc[len(F2C2_Blocked_Reactions_df)] = model_rxns\n",
    "\n",
    "# Identify unblocked reactions from the second row (index 1)\n",
    "unblocked_mask = F2C2_Blocked_Reactions_df.loc[0] == 0\n",
    "unblocked_reactions = F2C2_Blocked_Reactions_df.loc[1][unblocked_mask].tolist()\n",
    "\n",
    "# Update Coupled_Pairs_df index and columns with unblocked reactions\n",
    "Coupled_Pairs_df.index = unblocked_reactions\n",
    "Coupled_Pairs_df.columns = unblocked_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframe for reactions in model\n",
    "# Step 1: Get all reaction IDs from your COBRApy model\n",
    "model_rxns = set([rxn.id for rxn in consistent_generic_model.reactions])\n",
    "\n",
    "# Step 2: Identify which rows/columns are in the model\n",
    "valid_rxns = Coupled_Pairs_df.index.intersection(model_rxns)\n",
    "\n",
    "# Step 3: Subset the DataFrame to keep only those reactions\n",
    "Coupled_Pairs_df = Coupled_Pairs_df.loc[valid_rxns, valid_rxns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of how many fully coupled pairs\n",
    "fully_coupled_count = int(((Coupled_Pairs_df == 1).sum().sum()) - len(Coupled_Pairs_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SideQuest* : Investigate difference in 'blocking' between FASTCC and F2C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and convert reactions from models to sets for fast comparison\n",
    "generic_rxns = {rxn.id for rxn in consistent_generic_model.reactions}\n",
    "f2c2_unblocked_rxns = set(unblocked_reactions)\n",
    "\n",
    "# Compare overlaps and differences\n",
    "overlap_generic = list(generic_rxns & f2c2_unblocked_rxns)\n",
    "\n",
    "# Reactions unblocked by F2C2 but removed by FASTCC\n",
    "missing_from_generic = list(f2c2_unblocked_rxns - generic_rxns)\n",
    "\n",
    "# Reactions kept by FASTCC but blocked by F2C2 (ideally empty)\n",
    "unexpected_in_generic = list(generic_rxns - f2c2_unblocked_rxns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Create 'Fully Coupled Clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph of fully coupled reactions\n",
    "G = nx.Graph()\n",
    "\n",
    "for row in Coupled_Pairs_df.index:\n",
    "    for col in Coupled_Pairs_df.columns:\n",
    "        if Coupled_Pairs_df.loc[row, col] == 1 and row != col:\n",
    "            G.add_edge(row, col)\n",
    "\n",
    "# Find connected components (fully coupled groups)\n",
    "coupled_groups = list(nx.connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reaction groups with net stoichiometry and input/output metabolites\n",
    "combined_data = []\n",
    "for group in coupled_groups:\n",
    "    group_reactions = list(group)\n",
    "\n",
    "    # Compute net stoichiometry\n",
    "    stoich = defaultdict(float)\n",
    "    for rxn_id in group_reactions:\n",
    "        rxn = consistent_generic_model.reactions.get_by_id(rxn_id)\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            stoich[met] += coeff\n",
    "\n",
    "    inputs = [met.id for met, coeff in stoich.items() if coeff < 0]\n",
    "    outputs = [met.id for met, coeff in stoich.items() if coeff > 0]\n",
    "\n",
    "    combined_data.append({\n",
    "        \"Coupled_Reactions\": group_reactions,\n",
    "        \"Num_Inputs\": len(inputs),\n",
    "        \"Num_Outputs\": len(outputs),\n",
    "        \"Input_Metabolites\": inputs,\n",
    "        \"Output_Metabolites\": outputs\n",
    "    })\n",
    "\n",
    "# Create initial DataFrame\n",
    "Fully_Coupled_df = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'EX_' reactions removed: 25\n",
      "'BIOMASS_Ec_iAF1260_core_59p81M' was removed: True\n",
      "Total reactions removed due to fastcc removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Flatten reaction lists to count how many should be removed\n",
    "all_reactions = Fully_Coupled_df['Coupled_Reactions'].explode()\n",
    "\n",
    "ex_removed_count = all_reactions.str.startswith('EX_').sum()\n",
    "biomass_removed_flag = 'BIOMASS_Ec_iAF1260_core_59p81M' in set(all_reactions)\n",
    "missing_set = set(missing_from_generic)\n",
    "missing_removed_count = all_reactions.isin(missing_set).sum()\n",
    "\n",
    "# Clean the reaction lists: remove EX_, biomass, and missing reactions\n",
    "Fully_Coupled_df['Coupled_Reactions'] = Fully_Coupled_df['Coupled_Reactions'].apply(\n",
    "    lambda rxns: [\n",
    "        r for r in rxns\n",
    "        if not r.startswith('EX_')\n",
    "        and r != 'BIOMASS_Ec_iAF1260_core_59p81M'\n",
    "        and r not in missing_set\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop empty groups\n",
    "Fully_Coupled_df = Fully_Coupled_df[Fully_Coupled_df['Coupled_Reactions'].str.len() > 0]\n",
    "\n",
    "# Report removals\n",
    "print(f\"Total 'EX_' reactions removed: {ex_removed_count}\")\n",
    "print(f\"'BIOMASS_Ec_iAF1260_core_59p81M' was removed: {biomass_removed_flag}\")\n",
    "print(f\"Total reactions removed due to fastcc removal: {missing_removed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster connectivity\n",
    "Fully_Coupled_df['Cluster'] = Fully_Coupled_df['Coupled_Reactions'].apply(\n",
    "    lambda rxn_list: are_reactions_interconnected(consistent_generic_model, rxn_list)\n",
    ")\n",
    "\n",
    "# Count number of reactions\n",
    "Fully_Coupled_df['Num_Reactions'] = Fully_Coupled_df['Coupled_Reactions'].apply(len)\n",
    "\n",
    "# Remove disconnected pairs of size 2\n",
    "Fully_Coupled_df = Fully_Coupled_df[~((Fully_Coupled_df['Num_Reactions'] == 2) & (Fully_Coupled_df['Cluster'] == False))]\n",
    "\n",
    "# Fix broken clusters with more than 2 reactions\n",
    "mask = (Fully_Coupled_df['Cluster'] == False) & (Fully_Coupled_df['Num_Reactions'] > 2)\n",
    "Fully_Coupled_df.loc[mask, 'Cleaned_Coupled_Reactions'] = Fully_Coupled_df.loc[mask, 'Coupled_Reactions'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(consistent_generic_model, rxns)\n",
    ")\n",
    "\n",
    "# Keep reactions as-is for connected groups\n",
    "Fully_Coupled_df['Cleaned_Coupled_Reactions'] = Fully_Coupled_df['Cleaned_Coupled_Reactions'].fillna(Fully_Coupled_df['Coupled_Reactions'])\n",
    "\n",
    "# Track removed reactions per group\n",
    "Fully_Coupled_df['Removed_Reactions'] = Fully_Coupled_df.apply(\n",
    "    lambda row: list(set(row['Coupled_Reactions']) - set(row['Cleaned_Coupled_Reactions'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final DataFrame for analysis\n",
    "selected_columns = ['Cleaned_Coupled_Reactions', 'Removed_Reactions']\n",
    "Lumping_Couples_df = Fully_Coupled_df[selected_columns].copy()\n",
    "\n",
    "# Drop single-reaction groups\n",
    "Lumping_Couples_df = Lumping_Couples_df[Lumping_Couples_df['Cleaned_Coupled_Reactions'].str.len() > 1]\n",
    "\n",
    "# Map reactions to subsystems\n",
    "subsystem_map = dict(zip(subsystem_df['iAF1260_BIGG'], subsystem_df['Subsystem']))\n",
    "Lumping_Couples_df['Subsystem'] = Lumping_Couples_df['Cleaned_Coupled_Reactions'].apply(\n",
    "    lambda rxn_list: list(set(subsystem_map.get(rxn, 'Unknown') for rxn in rxn_list))\n",
    ")\n",
    "\n",
    "# Add summary columns\n",
    "Lumping_Couples_df['Num_Subsystems'] = Lumping_Couples_df['Subsystem'].apply(len)\n",
    "Lumping_Couples_df['Num_Reactions'] = Lumping_Couples_df['Cleaned_Coupled_Reactions'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Lump fully coupled clusters into pseudo-reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lumped, translation_df = lump_reaction(consistent_generic_model, Lumping_Couples_df, reaction_column=\"Cleaned_Coupled_Reactions\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Removed 0 internal metabolites used only in pseudo-reactions.\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove orphaned metabolites used only in pseudo-reactions\n",
    "metabolites_to_remove = []\n",
    "\n",
    "for met in model_lumped.metabolites:\n",
    "    # Get all reactions involving this metabolite\n",
    "    associated_rxns = [rxn.id for rxn in met.reactions]\n",
    "    \n",
    "    # If the metabolite only appears in one reaction AND it's a pseudo-reaction\n",
    "    if len(associated_rxns) == 1 and associated_rxns[0].startswith(\"Pseudo_\"):\n",
    "        metabolites_to_remove.append(met)\n",
    "\n",
    "# Remove them from the model\n",
    "model_lumped.remove_metabolites(metabolites_to_remove)\n",
    "\n",
    "print(f\"🧹 Removed {len(metabolites_to_remove)} internal metabolites used only in pseudo-reactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 515\n",
      "Number of Reactions: 946\n",
      "Degrees of Freedom: 431\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(model_lumped)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(model_lumped.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model FBA objective: 0.951942\n",
      "Lumped model FBA objective:   0.951942\n",
      "✅ FBA optimum preserved.\n"
     ]
    }
   ],
   "source": [
    "sol_orig = consistent_generic_model.optimize()\n",
    "sol_lumped = model_lumped.optimize()\n",
    "\n",
    "print(f\"Original model FBA objective: {sol_orig.objective_value:.6f}\")\n",
    "print(f\"Lumped model FBA objective:   {sol_lumped.objective_value:.6f}\")\n",
    "\n",
    "if abs(sol_orig.objective_value - sol_lumped.objective_value) < 1e-6:\n",
    "    print(\"✅ FBA optimum preserved.\")\n",
    "else:\n",
    "    print(\"⚠️ FBA optimum changed — check lumping effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pseudo-reactions: 194\n",
      "Blocked pseudo-reactions: 0\n",
      "✅ All pseudo-reactions are feasible.\n"
     ]
    }
   ],
   "source": [
    "# Get only the pseudo-reactions\n",
    "pseudo_rxns = [rxn.id for rxn in model_lumped.reactions if rxn.id.startswith(\"Pseudo_\")]\n",
    "\n",
    "# Run FVA on those reactions\n",
    "fva_result = flux_variability_analysis(model_lumped, reaction_list=pseudo_rxns, fraction_of_optimum=0.0)\n",
    "\n",
    "# Check which pseudo-reactions are blocked (min=max=0)\n",
    "blocked = fva_result[(fva_result[\"minimum\"] == 0) & (fva_result[\"maximum\"] == 0)]\n",
    "\n",
    "print(f\"Total pseudo-reactions: {len(pseudo_rxns)}\")\n",
    "print(f\"Blocked pseudo-reactions: {len(blocked)}\")\n",
    "\n",
    "if not blocked.empty:\n",
    "    print(\"⚠️ Some pseudo-reactions are blocked:\")\n",
    "    print(blocked)\n",
    "else:\n",
    "    print(\"✅ All pseudo-reactions are feasible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsystem based lumping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Group by module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with subsystem mapping\n",
    "subsystem_map = dict(zip(subsystem_df['iAF1260_BIGG'], subsystem_df['Subsystem']))\n",
    "\n",
    "# Create working copy\n",
    "translation_cp_df = translation_df.copy()\n",
    "\n",
    "# Apply the logic inline, returning a tuple (Subsystem, Unambiguous_Subsystem), then split into columns\n",
    "translation_cp_df[['Subsystem', 'Unambiguous_Subsystem']] = translation_cp_df['Original_Reactions'].apply(\n",
    "    lambda reactions: (\n",
    "        (lambda subsystems: (\n",
    "            Counter(subsystems).most_common(1)[0][0] if subsystems else None,\n",
    "            True if len(set(Counter(subsystems).values())) == 1 and len(set(subsystems)) == 1 else False\n",
    "        ))([subsystem_map[rxn] for rxn in reactions if rxn in subsystem_map])\n",
    "    )\n",
    ").apply(pd.Series)\n",
    "\n",
    "# Create new rows from subsystem_df\n",
    "new_rows = pd.DataFrame({\n",
    "    'Pseudo_Reaction_ID': subsystem_df['iAF1260_BIGG'],\n",
    "    'Original_Reactions': [None] * len(subsystem_df),\n",
    "    'Subsystem': subsystem_df['Subsystem'],\n",
    "    'Unambiguous_Subsystem': [True] * len(subsystem_df)  # Set to True explicitly\n",
    "})\n",
    "\n",
    "# Append new rows to the main DataFrame\n",
    "translation_cp_df = pd.concat([translation_cp_df, new_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract all reaction IDs from model_lumped\n",
    "model_reaction_ids = [rxn.id for rxn in model_lumped.reactions]\n",
    "\n",
    "# Subset translation_df to only those rows with matching Pseudo_Reaction_ID\n",
    "subset_df = translation_cp_df[translation_cp_df['Pseudo_Reaction_ID'].isin(model_reaction_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Subsystem' and aggregate\n",
    "grouped_df = subset_df.groupby('Subsystem').agg(\n",
    "    Members=('Pseudo_Reaction_ID', list),\n",
    "    Member_Count=('Pseudo_Reaction_ID', 'count')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Remove modules that cannot be lumped\n",
    "\n",
    "Note: For Core, reference: *Ataman, et al. \"redGEM: Systematic reduction ... consistent core metabolic models.\" PLoS computational biology 13.7 (2017): e1005444.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forbidden_subsystems_list = ['S_Exchange','S_Unassigned','S_']\n",
    "\n",
    "Core_subsystems_list = ['S_GlycolysisGluconeogenesis','S_Pentose_Phosphate_Pathway','S_Citric_Acid_Cycle','S_Glyoxylate_Metabolism','S_Pyruvate_Metabolism','S_Oxidative_Phosphorylation']\n",
    "\n",
    "Remove_list = Forbidden_subsystems_list + Core_subsystems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset: rows to remove (core)\n",
    "Core_grouped_df = grouped_df[grouped_df['Subsystem'].isin(Remove_list)].copy()\n",
    "\n",
    "# Subset: rows to keep (non-core)\n",
    "Noncore_grouped_df = grouped_df[~grouped_df['Subsystem'].isin(Remove_list)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Connect to relevance for biomass reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metabolites required by the biomass reaction (educts)\n",
    "biomass_rxn = model_lumped.reactions.get_by_id(\"BIOMASS_Ec_iAF1260_core_59p81M\")\n",
    "biomass_educts = {met.id for met, coeff in biomass_rxn.metabolites.items() if coeff < 0}\n",
    "\n",
    "# Containers for new columns\n",
    "educts_list = []\n",
    "products_list = []\n",
    "internal_list = []\n",
    "biomass_outputs = []\n",
    "biomass_flags = []\n",
    "\n",
    "# Iterate over each subsystem group (each row)\n",
    "for _, row in Noncore_grouped_df.iterrows():\n",
    "    reaction_ids = row['Members']\n",
    "    stoich = defaultdict(float)\n",
    "\n",
    "    # Accumulate net stoichiometry across all reactions\n",
    "    for rxn_id in reaction_ids:\n",
    "        if rxn_id not in model_lumped.reactions:\n",
    "            continue\n",
    "        rxn = model_lumped.reactions.get_by_id(rxn_id)\n",
    "        for met, coeff in rxn.metabolites.items():\n",
    "            stoich[met.id] += coeff\n",
    "\n",
    "    # Classify metabolites\n",
    "    educt_met = [m for m, c in stoich.items() if c < 0]\n",
    "    product_met = [m for m, c in stoich.items() if c > 0]\n",
    "\n",
    "    # Biomass-relevant products\n",
    "    biomass_met = [m for m in product_met if m in biomass_educts]\n",
    "    biomass_relevant = len(biomass_met) > 0\n",
    "\n",
    "    # Append results\n",
    "    educts_list.append(educt_met)\n",
    "    products_list.append(product_met)\n",
    "    biomass_outputs.append(biomass_met)\n",
    "    biomass_flags.append(biomass_relevant)\n",
    "\n",
    "# Assign new columns to grouped_df\n",
    "Noncore_grouped_df['educt_met'] = educts_list\n",
    "Noncore_grouped_df['product_met'] = products_list\n",
    "Noncore_grouped_df['biomass'] = biomass_outputs\n",
    "Noncore_grouped_df['biomass_relevant'] = biomass_flags\n",
    "\n",
    "Noncore_grouped_df['Interconnected'] = Noncore_grouped_df['Members'].apply(\n",
    "    lambda rxns: are_reactions_interconnected(model_lumped, rxns)\n",
    ")\n",
    "\n",
    "Noncore_grouped_df.sort_values(by='Member_Count',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find largest subclusters that are interconnected for non-interconnected groups\n",
    "mask = (Noncore_grouped_df['Interconnected'] == False)\n",
    "\n",
    "Noncore_grouped_df.loc[mask, 'Members_Cleaned_Reactions'] = Noncore_grouped_df.loc[mask, 'Members'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(model_lumped, rxns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df['Members_Cleaned_Reactions_Number'] = Noncore_grouped_df['Members_Cleaned_Reactions'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df[\"Leftover_Members\"] = Noncore_grouped_df.apply(\n",
    "    lambda row: list(set(row[\"Members\"]) - set(row[\"Members_Cleaned_Reactions\"]))\n",
    "    if isinstance(row[\"Members_Cleaned_Reactions\"], list) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find second largest subclusters that are interconnected for non-interconnected groups\n",
    "\n",
    "mask = Noncore_grouped_df[\"Members_Cleaned_Reactions\"].notna()\n",
    "\n",
    "# Apply function only to rows where the mask is True\n",
    "Noncore_grouped_df.loc[mask, 'Leftover_Cleaned_Members'] = Noncore_grouped_df.loc[mask, 'Leftover_Members'].apply(\n",
    "    lambda rxns: get_largest_connected_subgroup(model_lumped, rxns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noncore_grouped_df['Leftover_Cleaned_Members_Number'] = Noncore_grouped_df['Leftover_Cleaned_Members'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lump Subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Truly interconnected subsystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to initially true ones\n",
    "copy1_df = Noncore_grouped_df[Noncore_grouped_df['Interconnected'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy1, translation_df = lump_reaction(model_lumped, copy1_df, reaction_column=\"Members\",verbose=False,label_type=\"Subsystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 364\n",
      "Number of Reactions: 575\n",
      "Degrees of Freedom: 211\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(copy1)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(copy1.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Largest Subclusters of non interconnected Subsystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2_df = Noncore_grouped_df[Noncore_grouped_df[\"Members_Cleaned_Reactions\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2, translation_df = lump_reaction(copy1, copy2_df, reaction_column=\"Members_Cleaned_Reactions\",verbose=False,label_type=\"Subsystem_Cluster1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoichiometric Matrix Rank: 140\n",
      "Number of Reactions: 171\n",
      "Degrees of Freedom: 31\n"
     ]
    }
   ],
   "source": [
    "# Get the stoichiometric matrix as a NumPy array\n",
    "stoich_dense = create_stoichiometric_matrix(copy2)  \n",
    "\n",
    "# Compute the rank of the stoichiometric matrix\n",
    "rank = np.linalg.matrix_rank(stoich_dense)\n",
    "\n",
    "# Degrees of freedom\n",
    "num_reactions = len(copy2.reactions)\n",
    "dof = num_reactions - rank\n",
    "\n",
    "# Output\n",
    "print(f\"Stoichiometric Matrix Rank: {rank}\")\n",
    "print(f\"Number of Reactions: {num_reactions}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for both models\n",
    "results_copy2, flux_activity_copy2, filtered_flux_copy2 = analyze_model(copy2, 'copy2',carb_list)\n",
    "results_model_copy2, flux_activity_model_copy2, filtered_flux_model_copy2 = analyze_model(model_copy2, 'model_copy2',carb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective_comparison_df = pd.concat([results_copy2, results_model_copy2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective_comparison_df.to_csv(raw_csv_path / 'iAF1260_context_FBA_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cobra_escher_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
